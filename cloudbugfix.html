<html>

<head>
  <title>cloud infrastructure that proposes verifiable bugfixes</title>
  <link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>ðŸ”˜</text></svg>">
  <link rel="stylesheet" href="style.css">
  <script type="text/javascript" src="posthog.js"></script>
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-coy.min.css">
</head>

<body>
  <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-core.min.js"></script>
  <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-armasm.min.js"></script>
  <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-fortran.min.js"></script>
  <p><a href="./">..</a></p>
  <p style="text-align:right">2024-03-01</p>
  <h1>cloud infrastructure that proposes verifiable bugfixes</h1>
  </p>

  <p>
    LLMs are thus far the fastest adopted technology in history after OpenAI reported that ChatGPT achieved 100 million users within 2 months of its release. Chief among the uses cases for LLMs is code generation. Today, we mostly generate code with LLMs using chat-like interfaces within a terminal, website, or code editor. However as impressive as LLMs are at coding today, any developer that has used LLMs for code generation knows that even with the right context the vast majority of the time LLMs don't generate code that works as-is. Instead the generated code either needs to be throughly massaged, or it serves as inspiration for the code we ultimately end up writing. In some cases, the LLM-generated code seems to come close to the desired solution and we verify it by perusing it and/or manually running it within our codebases. However, this rarely works. One Princeton <a href="https://arxiv.org/pdf/2310.06770.pdf">study</a> that tasked two different LLMs to solve 2,294 real GitHub issues found that Claude 2 and GPT-4 solved only 4.8% and 1.7% of the issues respectively.
  </p>
  <!-- <p>
    Let's explore a workflow to automatically verify LLM-generated bugfixes outside of a chat/visual interface where we can't see the mistakes made by LLMs. These types of workflows to programmatically verify LLM-generated code might not be the norm today, but they will become more attractive as the price of invoking LLMs goes down and the generative coding abilities of LLMs improve.
  </p> -->
  <!-- <p>
    And every time we debug with LLMs we <i>do</i> know what the expected behavior is. The LLM-generated code needs to fix the provided bug without introducing new ones Today, we debug errors with LLMs using chat-like interfaces within a terminal, website, or code editor and we verify
    the LLM's generated code by perusing it and/or manually running it within our codebase. However, a less painful
    debugging workflow is possible outside of a chat interface where we can't see the many mistakes made by the LLM.

    Let's take a closer look at how we are currently using LLMs to debug our code and then dive deeper into a potential setup that would allow LLMs to automatically debug our code for us.

    Picture a cloud infrastructure offering that can debug containerized applications for us by submitting pull requests that verifiably fix existing production bugs while following codebase conventions. This type of self-healing infrastructure allows developers to spend less time debugging and more time shipping new features while providing a more reliable experience for end-users. Let's go over how this cloud offering works, but first
  <p> -->
  <p>
    I'm particularly interested in automating the verification of the LLM code generated for debugging application errors because debugging has a binary outcome that is always verifiable- did the error go away and did any new errors arise? If we are able to programmatically verify if an LLM can debug and solve for an application error it doesn't matter if the LLM-generated code fails to resolve the error the majority of the time, we can run this workflow over and over again as software far from a chat interface.
  </p>
  <!-- <p>
    The developer often provides the stacktrace to the LLM when debugging and can sometimes add more context by including the code referenced in the stacktrace. LLMs tend to do better when the error generated belongs to a commonly used programming language and/or framework as training datasets from Reddit and Stackoverflow are likely richer for these cases. The LLM can often output instructions or sample code that the developer can implement to quickly test if the bugfix works. This ends up being a conversation with a lot of back and forth where code generated by the LLM is interleaved with suggestions on ways to resolve the error that may or may not work thus requiring a lot of trial and error from the developer.
  </p> -->

  <p class="nostyle-header">
  <h3>automatically verifying LLM-generated bugfixes</h3>
  <!-- <p>
    Today, we debug errors with LLMs using chat-like interfaces within a terminal, website, or code editor and we verify
    the LLM's generated code by perusing it and/or manually running it within our codebase. However, a less painful
    debugging workflow is possible outside of a chat interface where the mistakes made by the LLM are not visible to us.
  <p> -->
  <p>
    Automatically debugging exceptions within a containerized production application is possible with an opinionated cloud infrastructure and error monitoring setup that collects the necessary data to verify bugfixes and behind the scenes tasks an LLM to generate a bugfix for each of the recorded production errors. A pull request to the application's codebase is submitted <i>if and only if</i> a bugfix verifiably fixes a given error. Pull requests ideally run with unit tests that ensure there are no breaking changes or regressions. Finally, one of the maintainers can take a closer look at the LLM's proposal for a verified bugfix to a particular error and decide to discard it, further edit it or merge it.
  </p>
  <p>
    In order for the cloud infrastructure hosting the production application in question to verify the LLM-generated bugfixes, the platform needs to be able to parse and collect incoming requests and outgoing network traffic using eBPF (more on that later). The infrastructure offering can then create branches out of the main git branch of the application's codebase with potential bugfixes and deploy them to sandboxed development environments behind the scenes. Finally, the cloud offering replays the incoming requests for the sandboxed environments while simultaneously mocking the outgoing network traffic using previously recorded values from the time the error took place. If no error is thrown, the cloud offering opens a pull request for the maintainers to review. This is a rough sketch of what I am describing:
  </p>

  <img src="./llmcodever.jpg"></img>

  <p>
    <!-- The bottom line is that we shouldn't have to peruse, or manually run, an LLM's generated code to know if it failed. We can evaluate the
    LLM's generated code programmatically even if at present LLMs will fail most of the time behind the scenes. However, as the
    capabilities of
    generative AI models to retain more context increases and their lazy tendency to produce incomplete code or
    hallucinate decreases, the hit rate will go up and the self-healing capabilities of such an abstraction will become more
    robust and valuable. -->

    A cloud infrastructure platform that is able to generate and verify LLM bugfixes
    while also providing the best UX requires two important features:
  </p>
  <p class="nostyle-header">
    1. git-based deployment process for containerized apps
  </p>
  <p>
    A git-centered deployment experience allows us to deploy by pushing a git branch to a remote repository without having to configure a CI/CD system.
    This workflow has been around since 2009 thanks to Heroku, the grandfather of all PaaS. This DevOps abstraction tucks away a fair amount of
    complexity by making some assumptions about our app. For most cases, it really should be
    as simple as some form of pushing to a branch with a <code>Dockerfile</code> or <code>docker-compose.yml</code> in it (or nothing in particular if using <a href="https://nixpacks.com/docs">Nixpacks</a>)
    without having to deal with the fairly standard DevOps process of:
    <li>building the docker image from a <code>Dockerfile</code></li>
    <li>pushing the image to a repository</li>
    <li>updating the process in a VM or K8 pod with the new code</li>
  </p>
  <p>
    I do wish more container cloud offerings today where as easy to use as the git-based,
    webapp offerings of Netlify or Vercel. <a href="https://fly.io/docs/languages-and-frameworks/dockerfile/">Fly.io</a>
    is one good example of a provider with a seamless,
    git-based deployment process for containerized apps.

    Cloud infrastructure that abstracts away the repetitive parts of the deployment process not only has superior UX,
    but it
    can easily provide features that can save us a lot of development time like <a href="https://vercel.com/docs/deployments/preview-deployments">preview deployments</a>.
    Most importantly, a git-based deployment process
    also makes it
    possible to programmatically replicate deployments behind the scenes. This is imperative to be able to propose verifiable bugfixes as we can test if an LLM can
    generate code that fixes a given bug, and do
    that repeatedly for all bugs, without an overly complicated CI/CD workflow, or some sort of spaghetti of yaml.
  </p>

  <p class="nostyle-header">
    2. replay of incoming traffic and mocking of outgoing traffic via eBPF-based, SDK-less error monitoring
  </p>

  <!-- <p>
    Highlight, DataDog and Sentry have led the charge in error monitoring. I have personally been a happy Sentry user
    for years. Their UI to triage, resolve and dive into errors does the job. Admittedly, the times where I have had to
    manually hook up a CI/CD to Sentry and programatically pass on information about <a href="https://docs.sentry.io/product/integrations/deployment/">deployments</a>, <a href="https://docs.sentry.io/product/releases/">releases</a>, and <a href="https://docs.sentry.io/platforms/javascript/sourcemaps/">compilation source maps</a>
    across multiple environments it has been a massive
    pain. However, Sentry's setup works particularly well when using some sort of PaaS integration for it like the one
    offered by Vercel as it abstracts all of the complexity described above.
  <p> -->
  </p>
  <p>
    Error monitoring tools are used primordialy to triage applicaiton errors. However, some offerings can also collect networking traffic to later replicate production conditions in sandboxed staging environments to improve reliability of upcoming software releases. Most error monitoring tools offer SDKs to explicitly instrument errors and networking calls within an app which can be tedious. However, some providers like
    <a href="https://www.datadoghq.com/knowledge-center/ebpf/#how-to-enjoy-the-benefits-of-ebpf-today">DataDog</a> have
    SDK-less error monitoring offerings built on <a href="https://ebpf.io">eBPF</a>.
  </p>
  <p>
    eBPF is a Linux technology that
    can run sandboxed programs in the kernel without changing kernel source. This technology opens the door for powerful error
    monitoring tools by hooking directly into kernel OS calls without any additional syntax required
    within a containerized codebase. However, fully SDK-less error monitoring via eBPF programs requires privileged, or
    <code>sudo</code>, access to hook into the kernel to then span the containerized app as a child process. This
    can be a complicated setup as most containerized deployment systems provide a sandboxed, or unprivileged,
    environment to run apps on.
    As such, SDK-less error monitoring is less common and typically reserved for business critical high performance needs.

    However, an SDK-less setup provides both a better UX, and the ability
    to replay incoming traffic and mock outgoing traffic behind the
    scenes which is necessary to verify the
    LLM's proposed bugfixes programmatically. Mocking all outgoing network traffic is particulally tricky as any communication to downstream databases and services needs to be properly parsed, recorded and stubbed which would not be possible without eBPF hooks.
  </p>

  <p class="nostyle-header">
    <!-- the verification of LLM-generated code will create a new suite of devtools -->
    <!-- the ramifications of verifying LLM-generated code in devtooling -->
    <!-- the ramifications of being able to verify LLM-generated code in devtooling -->
  <h3>devtools that can automatically verify LLM-generated code</h3>
  </p>
  <p>
    Certain devtools within domains where the functionality of LLM-generated code can be verified have been on the rise as they can save developers countless of hours. This trend will continue as the price of invoking LLMs goes down and the generative coding abilities of LLMs improve. <a href="https://www.figma.com/community/search?resource_type=plugins&sort_by=relevancy&query=Figma+to+code&editor_type=all&price=all&creators=all">Figma to code plugins</a> are now abundant and significantly better than only a couple of years ago thanks to the newer generative AI models that can understand design assets and use them to verify the correctness of
    LLM-generated frontend code. Similarly, we can verify code generated by an LLM that was
    tasked to fix application errors because the expected output is simply that the error goes away and no new errors
    arise.
  </p>
  <p>
    Drop me a line at lfdepombo at gmail dot com if you want to learn more about a cloud infrastructure offering I have been working on that proposes
    verifiable bugfixes. I would also love to hear your thoughts on any type of programmatic verification for
    LLM-generated
    code, or on
    building abstractions with LLMs
    without directly exposing a chat, or visual, interface to end-users.
  </p>

  <br>
  <br>
  <br>

</body>

</html>