<html>

<head>
  <title>cloudonic</title>
  <link rel="icon"
    href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>ðŸ”˜</text></svg>">
  <link rel="stylesheet" href="style.css">
  <script type="text/javascript" src="posthog.js"></script>
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-coy.min.css">
</head>

<body>
  <script type="text/javascript"
    src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-core.min.js"></script>
  <script type="text/javascript"
    src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-armasm.min.js"></script>
  <script type="text/javascript"
    src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-fortran.min.js"></script>
  <p><a href="./">..</a></p>
  <p style="text-align:right">2024-02-24</p>
  <h1>cloud infrastructure that proposes bugfixes</h1>
  <p>
    The implications of a type of cloud infrastructure for containerized applications that can submit pull requests that
    verifiably fix existing production bugs while following codebase conventions are vast. For developers, using this
    type of self-healing infrastructure can mean spending significantly less time debugging, allowing us to focus on
    building new features and shipping faster. And for end-users, a smoother, more reliable experience.
  <p>
  </p>
  <p>
    The premise of self-healing infrastructure requires that the functionality of LLM generated code
    <i>verifiably</i> fixes a
    particular bug without introducing new ones. This type of verification is hard to consistently do with LLMs in
    general. LLMs can
    process a lot of unstructured information and quickly generate code faster than humans, but it is often
    code that is not production ready, or may not even run at all. It can be straightforward to programmatically verify
    if the code generated by an LLM is using valid syntax. However, the harder part is to programmatically verify that
    the generated code follows the functionality described to the LLM. Let's go over how we might be able to do that for
    LLM generated bugfixes by starting first with how we are currently using LLMs to debug.
  </p>
  <p class="nostyle-header">
    what is the best interface or abstraction for debugging with LLMs?
    <br>
    ----------------------------------------------------------------------------------------
  <p>
    Today, we debug errors with LLMs using chat-like interfaces within a terminal, website or code editor and we verify
    the LLM's generated code by perusing it and/or manually running it within our codebase. However, a more
    productive debugging workflow is possible outside of a chat interface where the mistakes made by the LLM are not
    visible to us.
  <p>
  </p>
  <p>
    With the right infrastructure setup, we can replay production traffic at the time of an error into the sandboxed
    app running an LLM-generated potential bugfix of said error in order to verify the
    correctness of the code generated. The desired output of the workflow is
    that the error goes away and no new error is introduced. This is a rough sketch of what I am describing:
  </p>

  <img src="./cloudonic-workflow.jpg"></img>

  <p>
    We shouldn't have to peruse, or manually run, an LLM's generated code to know if it failed. We can evaluate the
    LLM's
    generated code programmatically as long as we know what the expected behavior of the generated code
    should be based on a previously determined input.
  <p>
  </p>
  <p>
    The LLM agent won't be able to fix most errors, but consequently we only see a pull request <i>if and only if</i>
    the code
    generated by the LLM is able to verifiably resolve the given error without introducing new ones. This means
    that at present it won't be fully self-healing as it will fail most of the time behind the scenes, but as the
    capabilities of
    generative AI models to retain more context increases and their lazy tendency to produce incomplete code or
    hallucinate decreases, the hit rate will go up and the self-healing capabilities of such a system will become more
    robust and valuable.
  </p>

  <p class="nostyle-header">
    requirements for a verifiable LLM debugging loop behind the scenes
    <br>
    --------------------------------------------------------------------------------------------
  </p>
  <p>
    The type of cloud offering that can propose bugfixes automatically needs two important features to be able
    to provide the best UX and easily run a behind-the-scenes LLM debugging loop like the one described above, or in
    other words, to let the
    LLM passively do the debugging for us:
  </p>
  <p class="nostyle-header">
    1. git-based deployment process for containerized apps
  </p>
  <p>
    A git-centered deployment experience is one that doesn't require us to configure a CI/CD in order to deploy, but
    merely pushing to a git
    branch. This workflow
    has been
    around since
    2009
    thanks to Heroku, the grandfather of all PaaS. This DevOps abstraction is amazing as it hides away a lot of
    complexity
    that often does not need to be exposed. I do wish more container cloud offerings today where as easy to use as the
    webapp offerings of Netlify or Vercel. <a href="https://fly.io/docs/languages-and-frameworks/dockerfile/">Fly.io</a>
    is one good example of a provider with a seamless,
    git-based
    deployment process for containerized apps. For most cases, it should really be
    as simple as some form of pushing to a branch with a <code>Dockerfile</code> or <code>docker-compose.yml</code> in
    it
    without having to deal with the fairly standard DevOps process of:
    <li>building the docker image from a <code>Dockerfile</code></li>
    <li>pushing the image to a repository</li>
    <li>updating the process in a VM or K8 pod with the new code</li>

  <p>
  </p>
  <p>
    Cloud infrastructure that abstracts away the deployment process not only has a superior UX, but it also makes it
    possible to programmatically replicate deployments behind the scenes to
    test if an LLM can
    generate code that fixes a given bug, and do
    that repeatedly for all bugs, without an overly complicated CI/CD workflow, or some sort of spaghetti of yaml.</p>

  <p class="nostyle-header">
    2. automatic replay of production traffic via SDK-less error monitoring
  </p>

  <p>
    Highlight, DataDog and Sentry have led the charge in error monitoring. I have personally been a happy Sentry user
    for years. Their UI to triage, resolve and dive into errors does the job. Admittedly, the times where I have had to
    manually hook up a CI/CD to Sentry and programatically pass on information about <a
      href="https://docs.sentry.io/product/integrations/deployment/">deployments</a>, <a
      href="https://docs.sentry.io/product/releases/">releases</a>, and <a
      href="https://docs.sentry.io/platforms/javascript/sourcemaps/">compilation source maps</a>
    across multiple environments it has been a massive
    pain. However, Sentry's setup works particularly well when using some sort of PaaS integration for it like the one
    offered by Vercel as it abstracts all of the complexity described above.
  <p>
  </p>
  <p>
    Most error monitoring tools offer SDKs to instrument errors within an app. However, a
    few
    offerings like
    <a href="https://www.datadoghq.com/knowledge-center/ebpf/#how-to-enjoy-the-benefits-of-ebpf-today">DataDog's</a> has
    SDK-less
    error monitoring thanks to <a href="https://ebpf.io">eBPF</a> which was released in 2014. eBPF stands
    for
    extended Berkeley Packet
    Filter and it's a Linux technology that can run sandboxed programs in the kernel without changing kernel source.
    This technology opens the door for powerful error monitoring tools by hooking directly into kernel OS calls without
    any
    additional syntax required
    within a containerized codebase. However, fully SDK-less error monitoring via eBPF programs requires privileged, or
    <code>sudo</code>, access to hook into the kernel to then span the containerized app as a child process. This
    can be a complicated setup as most containerized deployment systems provide a sandboxed, or unprivileged,
    environment to run apps on.
    As such, SDK-less error monitoring is less common unless there is a business critical high performance need.
    That said, self-healing cloud infrastructure
    is best served by offering this SDK-less setup which provides both a better UX, and the ability
    to replay production traffic behind the
    scenes which is necessary to verify the
    LLM's proposed bugfixes programmatically.
  </p>

  <p class="nostyle-header">
    curious to see it in action?
    <br>
    ----------------------------------
  </p>
  <p>
    I am working on a cloud infrastructure offering that proposes bugfixes and I am going to be running
    a closed
    beta soon. If you are interested in trying it out and being a design partner it would be great to hear from you at
    <a href="https://cloudonic.dev">cloudonic.dev</a>
  </p>

  <br>
  <br>
  <br>

</body>

</html>